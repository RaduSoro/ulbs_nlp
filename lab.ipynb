{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "# nltk.download()\n",
    "import xml.etree.ElementTree as ET\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "import json\n",
    "import glob\n",
    "import re\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "from nltk.tokenize import TweetTokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "           Topic   Description\n1POL   CURRENT NEWS - POLITICS\n2ECO  CURRENT NEWS - ECONOMICS\n3SPO      CURRENT NEWS - SPORT\n4GEN    CURRENT NEWS - GENERAL\n6INS  CURRENT NEWS - INSURANCE",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Topic   Description</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1POL</th>\n      <td>CURRENT NEWS - POLITICS</td>\n    </tr>\n    <tr>\n      <th>2ECO</th>\n      <td>CURRENT NEWS - ECONOMICS</td>\n    </tr>\n    <tr>\n      <th>3SPO</th>\n      <td>CURRENT NEWS - SPORT</td>\n    </tr>\n    <tr>\n      <th>4GEN</th>\n      <td>CURRENT NEWS - GENERAL</td>\n    </tr>\n    <tr>\n      <th>6INS</th>\n      <td>CURRENT NEWS - INSURANCE</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 2
    }
   ],
   "source": [
    "df = pd.read_csv(\"./data/Lista codificare Topics in baza de date Reuters.txt\", sep=\"\\t\")\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "Tokenizer=TweetTokenizer()\n",
    "punctuation_tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "\n",
    "def save_json_data(filename, data):\n",
    "    with open(filename, 'w',) as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "def process_sentece(data):\n",
    "    #make all text lowerCase\n",
    "    data = data.lower()\n",
    "    data = Tokenizer.tokenize(data)\n",
    "    stop = stopwords.words('english')\n",
    "    #remove stopwords\n",
    "    data = [item for item in data if item not in stop]\n",
    "    #do the lemmatization\n",
    "    \n",
    "    #do the stemming\n",
    "    data = [stemmer.stem(y) for y in data]\n",
    "    #remove punctuation:\n",
    "    data = punctuation_tokenizer.tokenize(' '.join(data))\n",
    "    \n",
    "    return data\n",
    "\n",
    "def get_text_from_title(file):\n",
    "    tree = ET.parse(file)\n",
    "    return tree.find(\"title\").text\n",
    "\n",
    "def count_words_in_text(found_text):\n",
    "     wordfreq = {}\n",
    "     for word in process_sentece(found_text):\n",
    "            if word not in wordfreq:\n",
    "                wordfreq[word] = 0 \n",
    "            wordfreq[word] += 1\n",
    "     return wordfreq\n",
    "\n",
    "def process_title(filename):\n",
    "    wordfreq = {}\n",
    "    extracted_text = get_text_from_title(filename)\n",
    "    wordfreq = count_words_in_text(extracted_text)\n",
    "    return {\"word_freq\": wordfreq, \"extracted_text\": extracted_text}\n",
    "\n",
    "def topic_to_description(topic):\n",
    "    return df.loc[topic].values\n",
    "\n",
    "def topic_extractor(file):\n",
    "    topics = []\n",
    "    tree = ET.parse(file)\n",
    "    for element in tree.find(\"//codes[@class='bip:topics:1.0']\"):\n",
    "        topics.append(element.attrib['code'])\n",
    "    return topics\n",
    "\n",
    "def process_body(file,tag):\n",
    "    data = {}\n",
    "    tree = ET.parse(file)\n",
    "    root = tree.getroot()\n",
    "    for child in root.findall(tag):\n",
    "        found_text = \"\"\n",
    "        for element in child:\n",
    "            found_text += element.text + \"\\n \"\n",
    "            # print(element.text)\n",
    "        #count words after doing the sentence tokenizing    \n",
    "        data['word_freq'] = count_words_in_text(found_text)\n",
    "        data['extracted_text'] = found_text\n",
    "        return data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def process_file (path, name):\n",
    "    file_data = {}\n",
    "    file_data['text'] = process_body(path,\"text\")\n",
    "    file_data['total_word_count'] = len(process_body(path,\"text\")['extracted_text'])\n",
    "    file_data['title'] = process_title(path)\n",
    "    file_data['topics'] = {}\n",
    "    topics = topic_extractor(path)\n",
    "    for item in topics:\n",
    "        # add topic and its meaning\n",
    "        topic = topic_to_description(item)[0]\n",
    "        file_data['topics'][item] = topic\n",
    "    \n",
    "    #save data\n",
    "    save_json_data(f\"./data/processed/34/{name}.json\",file_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "d:\\labs master\\text mining - nlp\\venv\\lib\\site-packages\\ipykernel_launcher.py:43: FutureWarning: This search is broken in 1.3 and earlier, and will be fixed in a future version.  If you rely on the current behaviour, change it to \".//codes[@class='bip:topics:1.0']\"\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "#get an array of file path\n",
    "files = glob.glob(\"./data/Reuters_34/Training/*\")\n",
    "#process each file\n",
    "for path in files:\n",
    "    name = path.split(\"\\\\\")[1][:-4]\n",
    "    process_file(path,name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "['./data/processed/34\\\\2504NEWS.json', './data/processed/34\\\\2538NEWS.json', './data/processed/34\\\\2775NEWS.json', './data/processed/34\\\\2792NEWS.json', './data/processed/34\\\\2822NEWS.json', './data/processed/34\\\\2836NEWS.json', './data/processed/34\\\\2848NEWS.json', './data/processed/34\\\\2917NEWS.json', './data/processed/34\\\\2955NEWS.json', './data/processed/34\\\\2978NEWS.json', './data/processed/34\\\\2982NEWS.json', './data/processed/34\\\\2984NEWS.json', './data/processed/34\\\\2988NEWS.json', './data/processed/34\\\\3665NEWS.json', './data/processed/34\\\\3785NEWS.json', './data/processed/34\\\\3813NEWS.json', './data/processed/34\\\\3902NEWS.json', './data/processed/34\\\\4206NEWS.json', './data/processed/34\\\\4263NEWS.json', './data/processed/34\\\\4289NEWS.json', './data/processed/34\\\\4294NEWS.json', './data/processed/34\\\\5104NEWS.json', './data/processed/34\\\\5216NEWS.json', './data/processed/34\\\\5220NEWS.json', './data/processed/34\\\\5229NEWS.json', './data/processed/34\\\\5520NEWS.json', './data/processed/34\\\\5524NEWS.json', './data/processed/34\\\\5530NEWS.json', './data/processed/34\\\\5537NEWS.json', './data/processed/34\\\\5541NEWS.json', './data/processed/34\\\\5550NEWS.json', './data/processed/34\\\\5675NEWS.json', './data/processed/34\\\\5678NEWS.json', './data/processed/34\\\\5697NEWS.json']\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "def merge_dict(global_text_data,local_text_data):\n",
    "    for key in local_text_data.keys():\n",
    "        if key in global_text_data.keys():\n",
    "            global_text_data[key]+= local_text_data[key]\n",
    "        else:\n",
    "            global_text_data[key]= local_text_data[key]\n",
    "    return global_text_data\n",
    "\n",
    "def merge_dict_text(global_text_data,local_text_data):\n",
    "    for key in local_text_data.keys():\n",
    "        if key not in global_text_data.keys():\n",
    "            global_text_data[key]= local_text_data[key]\n",
    "      \n",
    "    return global_text_data\n",
    "#COMPILE GLOBAL DICTIONARY\n",
    "def mk_global_dict():\n",
    "    files = glob.glob(\"./data/processed/34/*\")\n",
    "    print(files)\n",
    "    global_json = None\n",
    "    for file in files:\n",
    "        f = open(file)\n",
    "        data = json.load(f)\n",
    "        # print(data)\n",
    "        if global_json is None:\n",
    "            global_json = data\n",
    "            # print(global_json)\n",
    "        else:\n",
    "            #add the word freq from text \n",
    "            global_text_data = global_json['text']['word_freq']\n",
    "            local_text_data = data['text']['word_freq']\n",
    "            global_text_data = merge_dict(global_text_data,local_text_data)\n",
    "            \n",
    "            #add title word counts\n",
    "            global_title_data = global_json['title']['word_freq']\n",
    "            local_title_data = data['title']['word_freq']\n",
    "            global_title_data = merge_dict(global_title_data,local_title_data)\n",
    "            #add topics\n",
    "            global_topics_data = global_json['topics']\n",
    "            local_topics_data = data['topics']\n",
    "            global_topics_data = merge_dict_text(global_topics_data,local_topics_data)\n",
    "            #update in ditionary\n",
    "            global_json['text']['word_freq'] = global_text_data\n",
    "            global_json['title']['word_freq'] = global_title_data\n",
    "            global_json['topics'] = global_topics_data\n",
    "    count = 0\n",
    "    for key in global_json['text']['word_freq'].keys():\n",
    "        count += global_json['text']['word_freq'][key]\n",
    "    global_json['unique_words'] = len(global_json['text']['word_freq'].keys())\n",
    "    global_json['unique_words_count'] = count\n",
    "    global_json['text']['word_freq'] = merge_dict(global_json['text']['word_freq'],global_json['title']['word_freq'])\n",
    "    del global_json['title']\n",
    "    del global_json['total_word_count']\n",
    "    del global_json['text']['extracted_text']\n",
    "    save_json_data(\"./data/processed/34/global.json\",global_json)\n",
    "        \n",
    "mk_global_dict()             \n",
    "#TO DO ADD TITLE TO STEMMING"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}